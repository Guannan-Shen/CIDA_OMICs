---
title: "HW9"
author: "Guannan Shen"
date: "November 29, 2018"
output: 
  pdf_document:
    latex_engine: lualatex
    number_sections: yes
    toc: yes
    toc_depth: 5
  word_document:
    toc: yes
    toc_depth: '5'
  html_document:
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 5
    toc_float: yes
---

```{r setup, include=FALSE, cache = FALSE}
require("knitr")
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
opts_chunk$set(engine = "R")
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)
## setting wd in DELL
## opts_knit$set(root.dir = "~/Stats/CIDA_OMICs/CIDA_OMICS/7659Stats_Genetics/HW5/")
## setting working directory in asus 
opts_knit$set(root.dir = "C:/Users/hithr/Documents/Stats/CIDA_OMICs/7659Stats_Genetics/HW9/") 
## setting working directory in ubuntu
## opts_knit$set(root.dir = "~/Documents/Stats/CIDA_OMICs/7659Stats_Genetics/HW9/")
                                                 
## cache = F, if cache = T, will not revaluate code chunk everytime
## double or more space to insert a line break
```


```{r libs}
## set up workspace
library(class)
library(knitr)
library(tidyverse)
library(magrittr)
library(stats)
options(stringsAsFactors = F)
options(dplyr.width = Inf)
getwd()
## not in function
'%nin%' <- Negate('%in%')

# ######## clean memory ######################
# rm(list = ls())
# gc()
# slotNames(x)
# getSlots(x)

```

# Classification
• Download the data provided on Canvas (dataHW9-breastcancer.Rdata). This file contains the "breastcancer" object, which is a dataset for 46 breast tumor samples where 23 are positive for an estrogen receptor (ER+) and 23 were negative (ER-) (West et al., PNAS 2001 98:11462-11467). There are expression levels for 7129 genes for each sample in this list (x) and class labels for each sample (y).  
  
• Install the class package from CRAN

## (a) Describe the k-nearest neighbor algorithm and how it classifies observations. Using the function knn() from the class package, run k-nearest neighbors with k = 3 to train and test on the same training data set (train). What percentage of subjects were correctly classified? It is not good practice to train and test on the same data set, why not?

The KNN here is a non-parametric method used for classification. The output is a class label. An object is classified by a majority vote of its k nearest neighbors, and the classes of neighbors are known. Thus, this is a type of supervised learning.  

This method using training examples with features and class labels. When a test point (have features but does not have a class label) is provided, the distance metric (such as Euclidean distance, correlation coefficients) is calculated to define its k nearest neighbors. Eventually, the test point is assigned to the majority labels of its neighbors. 

```{r knn}
load("dataHW9_breastcancer.Rdata")
## import train test data
train <-  breastcancer[[1]] #training expression data
trainclass <-  breastcancer[[2]] #training classes
test <-  newpatients #new expression data
testclass <-  trueclasses #new classes

## run knn()
k1 <- knn(t(train),t(train), trainclass, k = 3)
k1_err <- sum(!(k1 == trainclass))/length(k1)
k1_err

## k selection
# k is usually small
set.seed(123)
k_err <- NULL
k_range <- 1:20
for(i in k_range){
  knn = knn(t(train),t(train), trainclass, k = i)
  k_err[i] <- sum(!(knn == trainclass))/length(knn)
}

## double check
sum(!(knn(t(train),t(train), trainclass, k = 1) == trainclass))

## plot 
plot(k_range, k_err, type = "o")

### test with new data
k_err_test <- NULL
for(i in c(1,10)){
  knn = knn(t(train),t(test), trainclass, k = i)
  k_err_test[which(c(1,10) %in% i)] <- sum(!(knn == testclass))/length(knn)
}
k_err_test


```

In this case, `r (1 - k1_err)*100`% of subjects were correctly classified.  

If we train and test on the same data set, the result always looks too good, thus we can not test how well the model actually is. And we would never know if this model is generalizable or not. This approach cannot adjust for the overfitting issue.

## (b) Repeat part a) but for multiple values of k. What values and range of k are suitable for k-nearest neighbor? Plot k versus error rate. What value of k would you select based on this plot and why?

The k is usually a small number, but larger values of k reduces effect of the noise on the classification. Also, in binary (two class) classification problems, it is helpful to choose k to be an odd number as this avoids tied votes. Since I don't fully trust the train and test with the same dataset approach. In this case, based on the above plot, I would choose k = 1 or k = 10 instead of one k. 

## (c) Using k selected in part b), predict the tumor class for three new subjects in newpatients using their k nearest-neighbors in the training data. The correct classes are in trueclasses. How well did you do?

In my case, k=1 has an error rate `r k_err_test[1]` and k=10 has an error rate `r k_err_test[2]`. 

```{r moreknn}
## 5 fold cross validation




```

## (d) Extra credit: Perform 5-fold cross validation to determine your error rate on the training data (perform on only one random partitioning). Since 46/5 is not an integer, make sure that the test group is the one with 10 patient samples. Plot your results for different values of k. How does this compare to the error rates from part a) above.


# Clustering
Download the data provided on Canvas (dataHW9-cellcycle.txt) for yeast gene expression over two cell cycles (Cho et al., Molecular Cell 1998, 2:65-73). In this experiment, mRNA was extracted from yeast cells at 10 minute intervals after reinitiation of the cell cycle. The data entries are fluorescence intensities for a single dye at each time point. The first column is the yeast gene ID and the second column is the gene name. HSP genes encode heat shock proteins, RPS genes encode ribosomal proteins and MCM genes encode for members of the MCM (mini-chromosome maintenance) complex.  

Read sections 10.4 and 11.5.1 provided on Canvas (Computational Genome Analysis, Deonier, Tavare & Waterman, 2005).  

The stats package contains both the hclust() and kmeans() functions.

```{r unsuper}
# load txt data
yeast_cellcycle <- read.delim("dataHW9-cellcycle.txt", header = F, sep = "\t")
yeast <- yeast_cellcycle[,3:18]
rownames(yeast) <- yeast_cellcycle[,2]
########### k means method ############
## standardized data for k means 
syeast <- apply(yeast, 1, function(x){
  (x-mean(x))/sd(x)
})

## kmeans and try k
maxk <- 7
k_c <- 1:maxk
k_sws <- NULL

k_wsm <- NULL
for(i in k_c){
km <- kmeans(t(syeast), i, iter.max = 10)
k_sws[i] <- sum(km$withinss)
 if(i == 1){
   k_wsm <- c(km$withinss)
 }else{
   k_wsm <- c(k_wsm, km$withinss)
 }
}
plot(k_c, k_sws, type = "o", xlab = "Number of Clusters", ylab = "Sum of Within cluster sum of squares")

## using all wsm 
## stacking bar plot
k_sall <- data.frame(matrix(NA, nrow = sum(1:maxk), ncol = 3))
k_sall$X1 <- as.factor(rep(1:maxk, 1:maxk))
k_sall$X2 <-  sapply(1:maxk, function(x) seq(1:x) ) %>% unlist() %>% as.factor()
k_sall$X3 <- k_wsm

ggplot(data=k_sall, aes(x=X1, y=X3, fill=X2)) +
  geom_bar(stat="identity") +
  labs(fill = "Clusters") +
  labs(x = "Number of Clusters") +
  labs(y = "Within Sum of Squares")

## choice of 4
km4 <- kmeans(t(syeast), 4, iter.max = 10)
km4$cluster
  
############### hierarchical clustering ############
hclust()

```





